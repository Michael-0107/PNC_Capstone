{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Use version 1.x not 2.x\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    records = []\n",
    "    for company, periods in data.items():\n",
    "        for period, (features, rating, normed_rating) in periods.items():\n",
    "            record = {\n",
    "                'company': company,\n",
    "                'period': period,\n",
    "                **{f'feature_{i}': feature.item() for i, feature in enumerate(features)},\n",
    "                'rating': rating.item(),\n",
    "                'normed_rating': normed_rating.item()\n",
    "            }\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"Ret\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dict: 49\n",
      "output_dict: 37\n",
      "merged_dict: 37\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import CompustatExtractor\n",
    "importlib.reload(CompustatExtractor)\n",
    "from CompustatExtractor import CompustatExtractor\n",
    "import os\n",
    "import Hypers\n",
    "\n",
    "features = CompustatExtractor.process_compustat_features2(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"features_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"features_{suffix}_1\",\n",
    "\tadd_cpi=False\n",
    ")\n",
    "\n",
    "ratings = CompustatExtractor.process_compustat_ratings(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"ratings_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"ratings_{suffix}_1\"\n",
    ")\n",
    "\n",
    "merged_dict = CompustatExtractor.merge_input_output_dicts(\n",
    "\tfeatures,\n",
    "\tratings,\n",
    "\tsave=True,\n",
    "\tfilestem=f\"dataset_{suffix}_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "merged_dict = utils.load_pickle(os.path.join(Hypers.Config.data_path, f\"dataset_{suffix}_1.pkl\"))\n",
    "_, _ = utils.spilt_train_valid(merged_dict, random_select=True, save=True, suffix=(suffix + \"_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "import os\n",
    "import Hypers\n",
    "importlib.reload(Hypers)\n",
    "\n",
    "train_df = load_data('./data/train_dict_' + suffix + '_1.pkl')\n",
    "test_df = load_data('./data/test_dict_' + suffix + '_1.pkl')\n",
    "\n",
    "X_train = train_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_train = train_df['normed_rating']\n",
    "labels_train = train_df['rating']\n",
    "X_test = test_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_test = test_df['normed_rating']\n",
    "labels_test = test_df['rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>rating</th>\n",
       "      <th>normed_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7974.957674</td>\n",
       "      <td>12537.306597</td>\n",
       "      <td>20512.264269</td>\n",
       "      <td>1651.452361</td>\n",
       "      <td>8293.912846</td>\n",
       "      <td>5055.163610</td>\n",
       "      <td>7302.773576</td>\n",
       "      <td>6156.931230</td>\n",
       "      <td>13459.169889</td>\n",
       "      <td>387.379591</td>\n",
       "      <td>10253.399661</td>\n",
       "      <td>11439.442905</td>\n",
       "      <td>7040.365607</td>\n",
       "      <td>2220.053211</td>\n",
       "      <td>3.142466</td>\n",
       "      <td>0.285679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11790.918996</td>\n",
       "      <td>26425.135540</td>\n",
       "      <td>37823.527810</td>\n",
       "      <td>2803.746359</td>\n",
       "      <td>16578.767081</td>\n",
       "      <td>8264.905577</td>\n",
       "      <td>13278.726847</td>\n",
       "      <td>10318.598069</td>\n",
       "      <td>23270.499122</td>\n",
       "      <td>804.159056</td>\n",
       "      <td>22138.886139</td>\n",
       "      <td>22460.420172</td>\n",
       "      <td>14816.228569</td>\n",
       "      <td>4380.393280</td>\n",
       "      <td>1.090666</td>\n",
       "      <td>0.099151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>146.431000</td>\n",
       "      <td>285.282990</td>\n",
       "      <td>763.768005</td>\n",
       "      <td>1.664000</td>\n",
       "      <td>70.504997</td>\n",
       "      <td>83.723999</td>\n",
       "      <td>116.002998</td>\n",
       "      <td>297.992004</td>\n",
       "      <td>556.700012</td>\n",
       "      <td>-2640.000000</td>\n",
       "      <td>130.647003</td>\n",
       "      <td>169.832993</td>\n",
       "      <td>-1895.224976</td>\n",
       "      <td>92.889000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2076.648743</td>\n",
       "      <td>1606.512207</td>\n",
       "      <td>3669.390259</td>\n",
       "      <td>104.910503</td>\n",
       "      <td>1181.624023</td>\n",
       "      <td>1252.358032</td>\n",
       "      <td>1387.672485</td>\n",
       "      <td>897.888504</td>\n",
       "      <td>2380.476685</td>\n",
       "      <td>61.495250</td>\n",
       "      <td>1172.849213</td>\n",
       "      <td>1947.826477</td>\n",
       "      <td>1026.929260</td>\n",
       "      <td>375.636497</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3497.637451</td>\n",
       "      <td>3694.300049</td>\n",
       "      <td>7932.032471</td>\n",
       "      <td>682.500000</td>\n",
       "      <td>2373.177490</td>\n",
       "      <td>2529.595947</td>\n",
       "      <td>2737.000000</td>\n",
       "      <td>2329.400024</td>\n",
       "      <td>5876.873047</td>\n",
       "      <td>156.984497</td>\n",
       "      <td>2301.135498</td>\n",
       "      <td>3371.449951</td>\n",
       "      <td>2287.150024</td>\n",
       "      <td>740.002502</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10108.500000</td>\n",
       "      <td>12058.100098</td>\n",
       "      <td>23234.750000</td>\n",
       "      <td>1638.500000</td>\n",
       "      <td>8654.750000</td>\n",
       "      <td>5547.500000</td>\n",
       "      <td>8978.000000</td>\n",
       "      <td>5760.000000</td>\n",
       "      <td>14299.000000</td>\n",
       "      <td>372.031494</td>\n",
       "      <td>9009.750000</td>\n",
       "      <td>13055.250000</td>\n",
       "      <td>6391.750000</td>\n",
       "      <td>2506.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67142.000000</td>\n",
       "      <td>144353.000000</td>\n",
       "      <td>209876.000000</td>\n",
       "      <td>25981.000000</td>\n",
       "      <td>96823.000000</td>\n",
       "      <td>51501.000000</td>\n",
       "      <td>77021.000000</td>\n",
       "      <td>56605.000000</td>\n",
       "      <td>129862.000000</td>\n",
       "      <td>6056.000000</td>\n",
       "      <td>118723.000000</td>\n",
       "      <td>131565.000000</td>\n",
       "      <td>85937.000000</td>\n",
       "      <td>25993.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_0      feature_1      feature_2     feature_3     feature_4  \\\n",
       "count    730.000000     730.000000     730.000000    730.000000    730.000000   \n",
       "mean    7974.957674   12537.306597   20512.264269   1651.452361   8293.912846   \n",
       "std    11790.918996   26425.135540   37823.527810   2803.746359  16578.767081   \n",
       "min      146.431000     285.282990     763.768005      1.664000     70.504997   \n",
       "25%     2076.648743    1606.512207    3669.390259    104.910503   1181.624023   \n",
       "50%     3497.637451    3694.300049    7932.032471    682.500000   2373.177490   \n",
       "75%    10108.500000   12058.100098   23234.750000   1638.500000   8654.750000   \n",
       "max    67142.000000  144353.000000  209876.000000  25981.000000  96823.000000   \n",
       "\n",
       "          feature_5     feature_6     feature_7      feature_8    feature_9  \\\n",
       "count    730.000000    730.000000    730.000000     730.000000   730.000000   \n",
       "mean    5055.163610   7302.773576   6156.931230   13459.169889   387.379591   \n",
       "std     8264.905577  13278.726847  10318.598069   23270.499122   804.159056   \n",
       "min       83.723999    116.002998    297.992004     556.700012 -2640.000000   \n",
       "25%     1252.358032   1387.672485    897.888504    2380.476685    61.495250   \n",
       "50%     2529.595947   2737.000000   2329.400024    5876.873047   156.984497   \n",
       "75%     5547.500000   8978.000000   5760.000000   14299.000000   372.031494   \n",
       "max    51501.000000  77021.000000  56605.000000  129862.000000  6056.000000   \n",
       "\n",
       "          feature_10     feature_11    feature_12    feature_13      rating  \\\n",
       "count     730.000000     730.000000    730.000000    730.000000  730.000000   \n",
       "mean    10253.399661   11439.442905   7040.365607   2220.053211    3.142466   \n",
       "std     22138.886139   22460.420172  14816.228569   4380.393280    1.090666   \n",
       "min       130.647003     169.832993  -1895.224976     92.889000    1.000000   \n",
       "25%      1172.849213    1947.826477   1026.929260    375.636497    2.000000   \n",
       "50%      2301.135498    3371.449951   2287.150024    740.002502    3.000000   \n",
       "75%      9009.750000   13055.250000   6391.750000   2506.000000    4.000000   \n",
       "max    118723.000000  131565.000000  85937.000000  25993.000000    6.000000   \n",
       "\n",
       "       normed_rating  \n",
       "count     730.000000  \n",
       "mean        0.285679  \n",
       "std         0.099151  \n",
       "min         0.090909  \n",
       "25%         0.181818  \n",
       "50%         0.272727  \n",
       "75%         0.363636  \n",
       "max         0.545455  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Regression Results\n",
      "Decision Tree       : Accuracy=0.3050  MSE=0.0140\n",
      "Random Forest       : Accuracy=0.3546  MSE=0.0085\n",
      "Linear Regression   : Accuracy=0.3830  MSE=0.0074\n",
      "k-NN (k=1)          : Accuracy=0.2837  MSE=0.0100\n",
      "SVR                 : Accuracy=0.4184  MSE=0.0060\n",
      "XGBoost             : Accuracy=0.3404  MSE=0.0096\n",
      "LightGBM            : Accuracy=0.4610  MSE=0.0056\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import Hypers\n",
    "\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=RANDOM_SEED),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'k-NN (k=1)': KNeighborsRegressor(n_neighbors=1),\n",
    "    'SVR': SVR(),\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=RANDOM_SEED),\n",
    "    'LightGBM': lgb.LGBMRegressor(random_state=RANDOM_SEED, verbose=-1)\n",
    "}\n",
    "\n",
    "print('Train-Test Split Regression Results')\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_rounded = np.round(y_pred * (len(Hypers.rating_to_category) - 1)).astype(int)\n",
    "    y_pred_rounded = np.clip(y_pred_rounded, 0, 23)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    accuracy = accuracy_score(labels_test, y_pred_rounded)\n",
    "\n",
    "    print(f'{model_name:20}: Accuracy={accuracy:.4f}  MSE={mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"RetInd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dict: 924\n",
      "output_dict: 392\n",
      "merged_dict: 342\n"
     ]
    }
   ],
   "source": [
    "features = CompustatExtractor.process_compustat_features2(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"features_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"features_{suffix}_1\",\n",
    "\tadd_cpi=False\n",
    ")\n",
    "\n",
    "ratings = CompustatExtractor.process_compustat_ratings(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"ratings_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"ratings_{suffix}_1\"\n",
    ")\n",
    "\n",
    "merged_dict = CompustatExtractor.merge_input_output_dicts(\n",
    "\tfeatures,\n",
    "\tratings,\n",
    "\tsave=True,\n",
    "\tfilestem=f\"dataset_{suffix}_1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "merged_dict = utils.load_pickle(os.path.join(Hypers.Config.data_path, f\"dataset_{suffix}_1.pkl\"))\n",
    "_, _ = utils.spilt_train_valid(merged_dict, random_select=True, save=True, suffix=(suffix + \"_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "import os\n",
    "import Hypers\n",
    "importlib.reload(Hypers)\n",
    "\n",
    "train_df = load_data('./data/train_dict_' + suffix + '_1.pkl')\n",
    "test_df = load_data('./data/test_dict_' + suffix + '_1.pkl')\n",
    "\n",
    "X_train = train_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_train = train_df['normed_rating']\n",
    "labels_train = train_df['rating']\n",
    "X_test = test_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_test = test_df['normed_rating']\n",
    "labels_test = test_df['rating']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Regression Results\n",
      "Decision Tree       : Accuracy=0.3879  MSE=0.0073\n",
      "Random Forest       : Accuracy=0.4327  MSE=0.0047\n",
      "Linear Regression   : Accuracy=0.3730  MSE=0.0064\n",
      "k-NN (k=1)          : Accuracy=0.4306  MSE=0.0087\n",
      "SVR                 : Accuracy=0.4932  MSE=0.0042\n",
      "XGBoost             : Accuracy=0.4192  MSE=0.0052\n",
      "LightGBM            : Accuracy=0.4470  MSE=0.0044\n"
     ]
    }
   ],
   "source": [
    "print('Train-Test Split Regression Results')\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_rounded = np.round(y_pred * (len(Hypers.rating_to_category) - 1)).astype(int)\n",
    "    y_pred_rounded = np.clip(y_pred_rounded, 0, 23)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    accuracy = accuracy_score(labels_test, y_pred_rounded)\n",
    "\n",
    "    print(f'{model_name:20}: Accuracy={accuracy:.4f}  MSE={mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dict: 2510\n",
      "output_dict: 1151\n",
      "merged_dict: 775\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import CompustatExtractor\n",
    "importlib.reload(CompustatExtractor)\n",
    "from CompustatExtractor import CompustatExtractor\n",
    "import os\n",
    "import Hypers\n",
    "\n",
    "features = CompustatExtractor.process_compustat_features2(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"features_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"features_{suffix}_1\",\n",
    "\tadd_cpi=False\n",
    ")\n",
    "\n",
    "concatenate_features = CompustatExtractor.concatenate_features(features, k=1)\n",
    "\n",
    "ratings = CompustatExtractor.process_compustat_ratings(\n",
    "\tos.path.join(Hypers.Config.data_path, \"WRDS\", f\"ratings_{suffix}.csv\"),\n",
    "\tsave=False,\n",
    "\tfilestem=f\"ratings_{suffix}_1\"\n",
    ")\n",
    "\n",
    "merged_dict = CompustatExtractor.merge_input_output_dicts(\n",
    "\tconcatenate_features,\n",
    "\tratings,\n",
    "\tsave=True,\n",
    "\tfilestem=f\"dataset_{suffix}_1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "merged_dict = utils.load_pickle(os.path.join(Hypers.Config.data_path, f\"dataset_{suffix}_1.pkl\"))\n",
    "_, _ = utils.spilt_train_valid(merged_dict, random_select=True, save=True, suffix=(suffix + \"_1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import importlib\n",
    "import os\n",
    "import Hypers\n",
    "importlib.reload(Hypers)\n",
    "\n",
    "train_df = load_data('./data/train_dict_' + suffix + '_1.pkl')\n",
    "test_df = load_data('./data/test_dict_' + suffix + '_1.pkl')\n",
    "\n",
    "X_train = train_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_train = train_df['normed_rating']\n",
    "labels_train = train_df['rating']\n",
    "X_test = test_df.drop(columns=['company', 'period', 'rating', 'normed_rating'])\n",
    "y_test = test_df['normed_rating']\n",
    "labels_test = test_df['rating']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split Regression Results\n",
      "Decision Tree       : Accuracy=0.4016  MSE=0.0138\n",
      "Random Forest       : Accuracy=0.4768  MSE=0.0088\n",
      "Linear Regression   : Accuracy=0.3499  MSE=0.0107\n",
      "k-NN (k=1)          : Accuracy=0.4301  MSE=0.0136\n",
      "SVR                 : Accuracy=0.4310  MSE=0.0090\n",
      "XGBoost             : Accuracy=0.4630  MSE=0.0091\n",
      "LightGBM            : Accuracy=0.4947  MSE=0.0090\n"
     ]
    }
   ],
   "source": [
    "print('Train-Test Split Regression Results')\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_rounded = np.round(y_pred * (len(Hypers.rating_to_category) - 1)).astype(int)\n",
    "    y_pred_rounded = np.clip(y_pred_rounded, 0, 23)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    accuracy = accuracy_score(labels_test, y_pred_rounded)\n",
    "\n",
    "    print(f'{model_name:20}: Accuracy={accuracy:.4f}  MSE={mse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
